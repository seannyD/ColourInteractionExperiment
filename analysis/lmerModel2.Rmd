---
title: "Colour experiment"
output: 
    pdf_document:
      latex_engine: xelatex
      toc: yes
---


\newpage

# Introduction

This analysis looks at the sign variants used in a colour naming game between signers of different sign languages meeting after 1 week of interaction and after 3 weeks of interaction.  The data was collected by Kang Suk Byun (Kang-Suk.Byun@mpi.nl).

The analysis tries to predict the relative frequency of each variant within a colour category in week 3, based on measures from week 1.

## Data

-  colour: Code of the target colour
-  colourName: English name of the target colour
-  sign: label for the variant produced
-  freq_week_1_total: Total number of occurances of the variant in the first week, across all colour contexts.
-  freq_week_4_total: Total number of occurances of the variant in the final week, across all colour contexts.
-  freq_week_1: Number of occurances of the variant used during the given target colour context in week 1.
-  freq_week_4: Number of occurances of the variant used during the given target colour context in the final week.
-  prop_week_1: Same as freq_week_1, but as a proportion of all variants used in the given colour context.
-  prop_week_4: Same as freq_week_4, but as a proportion of all variants used in the given colour context.
-  origin:  The origin language of the sign.  For many, identifying an origin is not possible, so is labelled "None"
-  iconic: Old variable
-  check: The number of times this variable was used in a checking turn.
-  indexical:  Is the variant non-indexical, indexical or indexical of the body?
-  inventedBy:  The name of the first signer to use this variant in the experiment.
-  TryMarked:  The number of times this sign was used in try-marking.
-  Teach:  The number of times this sign was explicitly taught.
-  averageLength_week_1:  Average time to produce the variant in milliseconds
-  averageTrialLength_week_1:  Average time for completing the trial for the given target colour.
-  BodyAnchor:  Is the variant body-anchored (redundant with 'indexical')

\newpage

## Poisson regression

This study uses a mixed effects regression model with poisson distributions.  Most standard regression analyses assume that the values they are trying to model come from a normal distribution, like this:

```{r echo=F}
set.seed(237)
hist(rnorm(1000, mean = 8, sd = 2), main='', xlab='Frequency', ylab='Count')
```

However, the main variable for this study is the frequency of sign variants, with a strong skew and many zero values:

```{r echo=F}
variants = read.csv('../data/processedData/variants_summary.csv', stringsAsFactors = F)
hist(variants$freq_week_4, main='', xlab='Frequency', ylab='Count', breaks=0:max(variants$freq_week_4))
```

Instead of using a normal distribution as the basis for the statistical model or transforming the data (which is difficult anyway because of the large number of zero counts), we can use a poisson distribution.  This also has the advantage of only predicting whole, non-negative numbers, which makes sense for this data because a variant can't be used half a time or a negative number of times.

# Load libraries
```{r warning=F, message=F}

library(ggplot2)
library(lme4)
library(Rmisc)
library(dplyr)
library(sjPlot)
library(gridExtra)
library(REEMtree)
```

```{r echo=F}

setwd("~/Documents/MPI/KangSukColours/ColourExperiment/analysis/")

getMEText = function(r,ef, wald=NULL, showWald=F){
  
  AIC = r[2,]$AIC
  loglikDiff = signif(diff(r$logLik),2)
  chi = round(r$Chisq[2],2)
  df = r$`Chi Df`[2]
  p = signif(r$`Pr(>Chisq)`[2],2)
  
  wald.text = ""
  
  if(!is.null(wald)){
    est = signif(wald[1],2)
    stder = signif(wald[2],2)
    t = signif(wald[3],2)
    wptext = ""
    wald.text =  paste("beta = ",est,",")
    if(showWald){
      if(!is.na(wald[4])){
      wptext = paste(", Wald p =",signif(wald[4],2))
      }
    wald.text = paste("beta = ",est,", std.err = ",stder, ", Wald t = ",t,wptext,';')
    }
  }
  
  begin = 'There was no significant'
  if(p <0.09){
    begin = "There was a marginal"
  }
  if(p < 0.05){
    begin = 'There was a significant'  
  }
  
  
  return(paste(begin,ef,"(",wald.text,"log likelihood difference =",
               loglikDiff,", df = ",df,", Chi Squared =", chi,", p = ",p,")."))
}

```

# Load data

```{r}
variants = read.csv('../data/processedData/variants_summary.csv', stringsAsFactors = F)
```

There is only 1 variant for `white'.  Therefore, we remove it from this statistical analysis.

```{r}
variants = variants[variants$colourName!='white',]
```

Some variants need to be removed:

```{r}
variants = variants[!variants$sign %in%
            c("SAME",
              "DIFFERENT",
              "DO NOT UNDERSTAND",
              "UNDERSTAND",
              "ME",
              "YOU",
              "YES",
              "NOT",
              "SIGNING"),]
```


Transform some variables.

```{r}


# The range of values for 'Teach' is very small:
table(variants$Teach)
# So we'll turn it into a binary category: 
#  variants that were never taught and variants that were
variants$Teach = as.factor(variants$Teach >0)

# Similar for checking 
variants$check.any = as.factor(variants$check>0)

variants$candidateUnderstanding.any = as.factor(variants$candidateUnderstanding>0)

variants$T0 = c("No","Check")[as.numeric(variants$check.any=="TRUE")+1]
variants$T0[variants$candidateUnderstanding.any=="TRUE"] = "Candidate Understanding"

# Collapse checks and candidate understandings
variants$T0[variants$T0 %in% c("Check","Candidate Understanding")] = "Yes"

variants$T0 = factor(variants$T0, levels = c("No","Yes"))

# ... and T-1

variants$T_minus_1.any = as.factor(variants$T_minus_1>0)

# Transform total frequency
variants$freq_week_1_total.logcenter = 
  log(variants$freq_week_1_total + 1)
variants$freq_week_1_total.logcenter = scale(variants$freq_week_1_total.logcenter)

# cut TryMarking into two categories
variants$TryMarked.any = as.factor(variants$TryMarked>0)

# transform length

# One extreme value is scaled down to the next highest number
variants$averageLength_week_1[
  variants$averageLength_week_1>30000] =
  max(variants$averageLength_week_1[
    variants$averageLength_week_1<20000])

variants$averageLength_week_1.logcenter = log(variants$averageLength_week_1)
variants$averageLength_week_1.logcenter =
  scale(variants$averageLength_week_1.logcenter)

variants$indexical = as.factor(variants$indexical)

# Make inventedBy deviation coding 
#  (deviation from the grand mean)
variants$inventedBy = as.factor(variants$inventedBy)
#contrasts(variants$inventedBy) = contr.sum(length(levels(variants$inventedBy)))


listofsigns = variants[,c("colourName","sign")]
write.csv(listofsigns,"../results/descriptive/ListOfVariants.csv", fileEncoding = 'utf-8')

```

# LMER models

Each model predicts the frequency of a variant in week 4, with a random intercept by colourName.  The random intercept allows some colours to have higher variant frequencies than others.  This is useful because we know that signs for some colours are converged on quickly, making their frequencies within those colours potentially higher.  In other words, the use of a particular variant to refer to a given colour is not entierly independent of the use of another variant to refer to the same colour.

Random slopes allow the strength of the effect of a factor to be different for each colour concept.  Only indexicality is theoretically relevant here: it is possible that an indexical strategy, particularly body-indexical signs, would be more effective for some colours than others.  For example, the body affords indexicality for black (hair) and red (tongue), but not green.  We can check whether this is true by comparing a baseline model to one with a random slope for indexicality.

```{r ranEfTest, cache=T}
# Optimiser adjustments
gcontrol = control=glmerControl(
  optimizer = 'bobyqa',
  optCtrl = list(maxfun = 1000000))

m0 = glmer(freq_week_4 ~ 
             1
           + (1 | colourName) , 
           data=variants, family=poisson,
           control=gcontrol)

m1 = glmer(freq_week_4 ~ 
             1
           + (1 + indexical || colourName) , 
           data=variants, family=poisson,
           control=gcontrol)
anova(m0,m1)
```

Indeed, a random slope for indexicality significantly improves the model.  It is difficult to think of a theoretical reason why different colours would be affected differently for other variables.  Given that random slopes make the model more complex, these are left out.

We begin with a null model and gradually add predictor variables, using model comparison to judge the significance of each variable.

```{r modelBits, cache=T}
# Null model
m0 = glmer(freq_week_4 ~ 
             1
           + (1 + indexical || colourName) , 
           data=variants, family=poisson,
           control=gcontrol)

mFrq = glmer(freq_week_4 ~ 
               1 +  
               freq_week_1_total.logcenter +
               (1 + indexical || colourName) , 
             data=variants, family=poisson,
             control=gcontrol)

mLen = glmer(freq_week_4 ~ 
               1 +  
               freq_week_1_total.logcenter +
               averageLength_week_1.logcenter +
               (1 + indexical || colourName) , 
             data=variants, family=poisson,
             control=glmerControl(
               optimizer = 'bobyqa',
               optCtrl = list(maxfun = 10000000)))

# add indexicality
mIndx = glmer(freq_week_4 ~ 
                1 + 
                freq_week_1_total.logcenter +
                averageLength_week_1.logcenter +
                indexical + 
                (1 + indexical || colourName) , 
              data=variants, family=poisson,
              control=glmerControl(
                optimizer = c('Nelder_Mead'),
                optCtrl = list(maxfun = 100000000)))

# add whether the variant is explicitly taught
mTeach = glmer(freq_week_4 ~ 
                 1 +  
                 freq_week_1_total.logcenter +
                 averageLength_week_1.logcenter +
                 indexical + 
                 Teach +
                 (1 + indexical || colourName) , 
               data=variants, family=poisson,
               control=glmerControl(
                 optimizer = c('Nelder_Mead','bobyqa'),
                 optCtrl = list(maxfun = 10000000)))

mTry = glmer(freq_week_4 ~ 
               1 +  
               freq_week_1_total.logcenter +
               averageLength_week_1.logcenter +
               indexical + 
               Teach +
               TryMarked.any +
               (1 + indexical || colourName) , 
             data=variants, family=poisson,
             control=gcontrol)

mChk = glmer(freq_week_4 ~ 
               1 +  
               freq_week_1_total.logcenter +
               averageLength_week_1.logcenter +
               indexical + 
               Teach +
               TryMarked.any +
               T0 +
               (1 + indexical || colourName) , 
             data=variants, family=poisson,
             control=gcontrol)

mTm1 = glmer(freq_week_4 ~ 
               1 +  
               freq_week_1_total.logcenter +
               averageLength_week_1.logcenter +
               indexical + 
               Teach +
               TryMarked.any +
               T0 +
               T_minus_1.any +
               (1 + indexical || colourName) , 
             data=variants, family=poisson,
             control=glmerControl(
                 optimizer = c('Nelder_Mead','bobyqa'),
                 optCtrl = list(maxfun = 10000000)))

mInv = glmer(freq_week_4 ~ 
               1 +  
               freq_week_1_total.logcenter +
               averageLength_week_1.logcenter +
               indexical + 
               Teach +
               TryMarked.any +
               T0 +
               T_minus_1.any +
               inventedBy +
               (1 + indexical || colourName) , 
             data=variants, family=poisson,
             control=gcontrol)
```

Note that the model `mTm1` throws a convergence warning. It suggests that the estimates were still changing to some extent when the estimation process finished running.  However, a more accurate estimation of the model graident shows that the model is actually converged suitably:

```{r}
relgrad <- with(mTm1@optinfo$derivs, 
                solve(Hessian,gradient))
max(abs(relgrad))
```


Test some interactions:

```{r inteactions, cache=T}
mT0xF = glmer(freq_week_4 ~ 
               1 +  
               freq_week_1_total.logcenter +
               averageLength_week_1.logcenter +
               indexical + 
               Teach +
               TryMarked.any +
               T0 +
               T_minus_1.any +
               inventedBy +
               T0:freq_week_1_total.logcenter +
               (1 + indexical || colourName) , 
             data=variants, family=poisson,
             control=gcontrol)

mTchxF = glmer(freq_week_4 ~ 
               1 +  
               freq_week_1_total.logcenter +
               averageLength_week_1.logcenter +
               indexical + 
               Teach +
               TryMarked.any +
               T0 +
               T_minus_1.any +
               inventedBy +
               T0:freq_week_1_total.logcenter +
               Teach:freq_week_1_total.logcenter +
               (1 + indexical || colourName) , 
             data=variants, family=poisson,
             control=gcontrol)

mTryXF = glmer(freq_week_4 ~ 
               1 +  
               freq_week_1_total.logcenter +
               averageLength_week_1.logcenter +
               indexical + 
               Teach +
               TryMarked.any +
               T0 +
               T_minus_1.any +
               inventedBy +
               T0:freq_week_1_total.logcenter +
               Teach:freq_week_1_total.logcenter +
               TryMarked.any:freq_week_1_total.logcenter +
               (1 + indexical || colourName) , 
             data=variants, family=poisson,
             control=gcontrol)

```

# Results

Model comparison test:

Results of the model comparison tests.  In the output below, there is a summary of the structure of each model, then a table with titles "Df", "AIC", "BIC" etc.  Each row represents an increasingly complex model, with each fixed effect being added one at a time.  The statistics describe how much better the model fits the data compared to the model above.  A significant result (low p-value) suggests that the fixed effect significantly improves the fit of the model, and therefore is a good predictor of frequency of variants in the first week.
  
```{r}
modelComparisonResults = 
  anova(m0,mFrq,mLen,mIndx,mTeach,mTry,mChk,
      mTm1,mInv,mT0xF,mTchxF, mTryXF)
modelComparisonResults
```

Choose a final model for the beta values.

```{r}
finalModel = mTryXF
```

Write model comparison results to file:

```{r}
mcr = as.data.frame(modelComparisonResults)
mcr$AIC = round(mcr$AIC,2)
mcr$BIC = round(mcr$BIC,2)
mcr$logLik = round(mcr$logLik,2)
mcr$deviance = round(mcr$deviance,2)
mcr$Chisq = round(mcr$Chisq,2)
mcr$Significance = ""
mcr$Significance[mcr$`Pr(>Chisq)`<0.05] = "*"
mcr$p = mcr$`Pr(>Chisq)`
mcr$p = round(mcr$p,2)
mcr$p[mcr$p<0.001] = "< 0.001"

mcr = mcr[,c("AIC","BIC","logLik","Chisq",'Chi Df','p',"Significance")]
names(mcr) = c("AIC","BIC","Log likelihood","χ2","Df",'p','')
mcr[is.na(mcr)] = ""

rownames(mcr) = c("Null model","Frequency","Length",
                  "Indexicality","Teach","Try-marking",
                  "T0","T-1",'Identity of first user',
                  "T0 x Frequency","T-1 x Frequency",
                  'Try-marking x Frequency')
write.csv(mcr, 
          "../results/inferential/ModelComparisonResults.csv",
          row.names = T)
```


\newpage

## Final model parameter summary 

This show the estimates for all parameters in the model.

```{r}
summary(finalModel)
```


Check the predictions.  Each point in the graph below is a variant.  The actual frequency in the 3rd week is on the horizontal axis.  Many have a frequency of 0: they don't appear.  The model's predictions are shown on the vertical axis.  The diagonal line is just a guide to show a perfect fit.  Most of the points lie on this line, suggesting that the model has fit the data well.

```{r}
plot(variants$freq_week_4, exp(predict(finalModel)),
     xlab="Actual frequency",
     ylab="Model prediction")
abline(0,1)
```

Here is a summary of whether a variant appeared or did not appear in the final week, and the model's predictions of whether they appeared or not:

```{r echo=F}
tx = table(
  exp(predict(finalModel))>=1,
  variants$freq_week_4>=1)
colnames(tx) = c("Not observed","Observed")
rownames(tx) = c("Predicted: Not observed",
                 "Predicted: Observed")
tx
```

These are the variants used in the final week:

```{r}
variants[variants$freq_week_4>0,c("colourName","sign", 'freq_week_4')]
```

There were `r sum(variants$freq_week_4>0)` variants that had a frequency of greater than 0 in the final week.  

The model predicts `r tx[2,2]` of these correctly, missing `r tx[1,2]` variants (`r paste(variants[variants$freq_week_4>0 & exp(predict(finalModel))<1,]$sign,collapse=", ")`).  

The model predicts a further `r tx[2,1]` variants should have been observed in the final week but were not observed in reality:

```{r echo=F}
variants[variants$freq_week_4==0 & exp(predict(finalModel))>=1,c("colourName","sign")]
```



## Random slopes results

We can plot the random slopes below:

```{r}
dotplot(ranef(finalModel))
```

The effect of indexicality is stronger for red and brown, and weaker for pink.

Check whether the coefficients are radically different with a full random effects structure:

```{r checkFullModel, cache=T}
finalModelFull = update(finalModel, ~. + 
                          (0 + Teach + check.any +
                             T_minus_1.any + inventedBy || colourName),
                        control=gcontrol)
cor.test(fixef(finalModel), fixef(finalModelFull))
```

The correlation is high, suggesting that the extra random slopes would not have a big effect on the results.

\newpage

# Summary

Here is a summary of the main results:

`r getMEText(anova(m0,mFrq), "main effect of frequency in week 1",summary(finalModel)$coef['freq_week_1_total.logcenter',])`  More frequent forms in the first week were more likely to be more frequent in the final week.

`r getMEText(anova(mFrq,mLen), "main effect of sign length",summary(finalModel)$coef['averageLength_week_1.logcenter',])`  Shorter signs were more frequent in the final week.

`r getMEText(anova(mLen,mIndx), "main effect of indexicality",summary(finalModel)$coef['indexicalyes',])` (beta for body-indexical = 0.853). 

`r getMEText(anova(mIndx,mTeach), "main effect of teaching",summary(finalModel)$coef['TeachTRUE',])`  Teaching increased the frequency of variants in the final week.

`r getMEText(anova(mTeach,mTry), "main effect of try marking",summary(finalModel)$coef['TryMarked.anyTRUE',])` Variants that were try marked more often were more frequent in the final week.

`r getMEText(anova(mTry,mChk), "main effect of of repair initiation (T0)",summary(finalModel)$coef['T0Yes',])`  Variants used in a repair initiation were more frequent in the final week.


`r getMEText(anova(mChk,mTm1), "main effect of use in a problem source turn (T-1)",summary(finalModel)$coef['T_minus_1.anyTRUE',])` 


`r getMEText(anova(mTm1,mInv), "main effect of first user (inventedBy)")`  Signs invented by the signer from Jordan and Nepal were more frequent in the final week.

`r getMEText(anova(mInv,mT0xF), "interaction between T0 and frequency",summary(finalModel)$coef['freq_week_1_total.logcenter:T0Yes',])` The effect of frequency only applies to variants not appearing in T0 turns.

`r getMEText(anova(mT0xF,mTchxF), "interaction between Teaching and frequency")` 

`r getMEText(anova(mTchxF,mTryXF), "interaction between Try marking and frequency")` 


```{r}

```



We also found some evidence that the effect of indexicality is more important for black and red, and less important for green and pink.

\newpage

# Graphs

```{r echo=F, eval=F}

## Teaching and Try marking

#Plot the interaction between teaching and try marking.  Variants are more frequent in the final week if they were both taught and try marked.  However, the second plot shows that there are actually relatively few taught variants, suggesting that the results may not generalise well.

#Plot is raw means with standard errors.

sumStats2 = summarySE(variants, measurevar="freq_week_4",
                      groupvars=c("TryMarked.any","Teach"))

sumStats2$TryMarked.any = c("No","Yes")[as.numeric(sumStats2$TryMarked.any)]
dodge <- position_dodge(width=0.5) 

main.plot <- ggplot(sumStats2,
	aes(x = TryMarked.any, y = freq_week_4, colour=Teach)) +
  geom_point(position=dodge, size=4) + 
  geom_line(aes(group=Teach),position=dodge) +
  geom_errorbar(aes(ymax=freq_week_4+se, ymin=freq_week_4-se), width=0.25,position=dodge) +
  xlab("Try Marked") +
  ylab("Frequency in final week") +
  coord_cartesian(ylim=c(0,6)) + 
  scale_color_discrete(breaks=c(TRUE,FALSE),
                       labels=c("Yes","No"),
                       name="Teach") +
  theme(text=element_text(size=18))
main.plot


ggplot(variants,
	aes(x = TryMarked.any, y = freq_week_4, colour=Teach)) + geom_jitter()

pdf("../results/descriptive/graphs/Interaction_Teach_TryMark.pdf", height=5, width=6)
main.plot
dev.off()

#The same plot, but in model estimates:

x = sjp.int(finalModel, show.ci = F, prnt.plot = F)
#x$plot.list[[1]]$data$x = c(0.9,1.1,1.9,2.1)
x$plot.list[[1]]$data$x = c(1,1,2,2)
x$plot.list[[1]]$coordinates$limits$x = c(0.8,2.2)
x$plot.list[[1]]$scales$scales[[1]] = scale_x_continuous(limits = c(0.8,2.2))

x$plot.list[[1]] + 
  scale_x_continuous(breaks=c(1,2),
                      labels=c("No","Yes")) +
    scale_colour_discrete(breaks=c("TRUE","FALSE"),
                     labels = c("Yes","No")) + 
  labs(title="", y = "Frequency in final week", x ="Try Marked" ) + 
  geom_point()
```



Plot of the marginal effects of the predicted probabilities for each fixed effect:

```{r warning=F}
x = sjp.glmer(mInv, 'eff', 
          vars=c('Teach',
                 'TryMarked.any',
                 'T0',
                 'freq_week_1_total.logcenter',
                 'averageLength_week_1.logcenter',
                 'T_minus_1.any'), 
          facet.grid = F,
          prnt.plot = F,
          show.ci = T)

freqi = 1
leni = 2
t0i = 5
teachi = 3
tryi = 4
tm1i = 6

# Rescale frequency back to real values
freq.m = x$plot.list[[freqi]]$data$x
x$plot.list[[freqi]]$data$x = 
  exp(freq.m* 
  attr(variants$freq_week_1_total.logcenter, 'scaled:scale') + 
  attr(variants$freq_week_1_total.logcenter, 'scaled:center') -1)

x$plot.list[[freqi]]$labels$x = "Frequency"
x$plot.list[[freqi]]$labels$title = "Frequency"

# Rescale length back to real values
len.m = x$plot.list[[leni]]$data$x
x$plot.list[[leni]]$data$x = 
  exp(len.m* 
  attr(variants$averageLength_week_1.logcenter, 'scaled:scale') + 
  attr(variants$averageLength_week_1.logcenter, 'scaled:center') -1)

#x$plot.list[[1]]$labels$title = "Indexicality"
#x$plot.list[[1]]$scales$scales[[1]]$labels = c("No","Yes","Body")

x$plot.list[[teachi]]$labels$title = "Teaching"
x$plot.list[[teachi]]$scales$scales[[1]]$labels = c("No","Yes")
x$plot.list[[teachi]]$coordinates$limits$x = c(0.5,2.5)

x$plot.list[[tryi]]$labels$title = "Try Marking"
x$plot.list[[tryi]]$scales$scales[[1]]$labels = c("No","Yes")
x$plot.list[[tryi]]$coordinates$limits$x = c(0.5,2.5)

x$plot.list[[t0i]]$labels$title = "T0"
x$plot.list[[t0i]]$scales$scales[[1]]$labels = c("No","Yes")
x$plot.list[[t0i]]$coordinates$limits$x = c(0.5,2.5)

x$plot.list[[tm1i]]$labels$title = "T-1"
x$plot.list[[tm1i]]$scales$scales[[1]]$labels = c("No","Yes")
x$plot.list[[tm1i]]$coordinates$limits$x = c(0.5,2.5)

x$plot.list[[leni]]$labels$title = "Length"
for(i in 1:6){
  x$plot.list[[i]]$coordinates$limits$y = c(0,1)
  if(i!=leni){
    x$plot.list[[i]]$theme = 
      list(
        panel.grid.minor.x = element_blank(),
        axis.text.y = element_blank(),
        axis.line.y=element_blank())
  } else{
    x$plot.list[[i]]$theme = list(panel.grid.minor.x = element_blank())
  }
  x$plot.list[[i]]$labels$y = element_blank()
}

x$plot.list[[leni]]$labels$y = "Predicted incidents for final week frequency"
x$plot.list[[leni]]$labels$x = "Length (ms)"

grx = grid.arrange(
             x$plot.list[[leni]]  + scale_x_log10(),
             x$plot.list[[freqi]]  + scale_x_log10(),
             x$plot.list[[tm1i]]+ 
               geom_point() +
               geom_errorbar(aes(ymin=lower,ymax=upper), width=0.2),
             x$plot.list[[t0i]]+ 
               geom_point() +
               geom_errorbar(aes(ymin=lower,ymax=upper), width=0.2),
             x$plot.list[[teachi]]+ geom_point()+
               geom_errorbar(aes(ymin=lower,ymax=upper), width=0.2),
             x$plot.list[[tryi]]+ geom_point()+
               geom_errorbar(aes(ymin=lower,ymax=upper), width=0.2), 
             widths=c(2.1,1.8,1.1,1.1,1.1,1.1))

pdf("../results/descriptive/graphs/EffectsPlot.pdf", width=8.5, height=3.5)
plot(grx)
dev.off()

```

\newpage

Plot the average frequency in the final week for signs invented by each signer:

```{r}
itx = prop.table(table(
  variants$inventedBy,
  variants$freq_week_4>0), margin=1)
barplot(t(itx))

ggplot(variants, 
       aes(x=inventedBy, fill=indexical)) +
  geom_bar(position="fill")


ggplot(variants, 
       aes(x=freq_week_1_total,
           y=averageLength_week_1,
           colour=inventedBy)) +
  geom_point() + scale_y_log10() 

```

\newpage

Plot interactions between frequency and sequential variables:

```{r echo=T, eval=T}
xi = sjp.int(finalModel, swap.pred = T, facet.grid = F,
             prnt.plot = F)

for(i in 1:3){
  freq.mx = xi$plot.list[[i]]$data$x
  xi$plot.list[[i]]$data$x = 
    exp(freq.mx* 
    attr(variants$freq_week_1_total.logcenter, 'scaled:scale') + 
    attr(variants$freq_week_1_total.logcenter, 'scaled:center') -1)
}

ggplot(xi$plot.list[[1]]$data, aes(x,y,colour=grp)) + 
  geom_line()  +
  xlab("Frequency") + 
  ylab("Predicted incidents for final week frequency") +
  theme(legend.position="top") +
  scale_color_discrete(name="T0")

xi$plot.list[[2]]$data$grp = c("No","Yes")[as.numeric(xi$plot.list[[2]]$data$grp)]

ggplot(xi$plot.list[[2]]$data, aes(x,y,colour=grp)) + 
  geom_line()  +
  xlab("Frequency") + 
  ylab("Predicted incidents for final week frequency") +
  theme(legend.position="top") +
  scale_color_discrete(name="Teach") +
  coord_cartesian(ylim=c(0,1))

xi$plot.list[[3]]$data$grp = c("No","Yes")[as.numeric(xi$plot.list[[3]]$data$grp)]

ggplot(xi$plot.list[[3]]$data, aes(x,y,colour=grp)) + 
  geom_line()  +
  xlab("Frequency") + 
  ylab("Predicted incidents for final week frequency") +
  theme(legend.position="top") +
  scale_color_discrete(name="Try Marked") +
  coord_cartesian(ylim=c(0,1))

```


```{r eval=F, echo=F}

ggplot(variants, aes(x=freq_week_1_total,
                     y=freq_week_4,
                     colour=Teach)) +
         geom_point()+
         stat_smooth(method = "gam", fullrange = T, se=F)

ggplot(variants, aes(x=freq_week_1_total,
                     y=freq_week_4,
                     colour=T0)) +
         geom_point()+
         stat_smooth(method = "gam", fullrange = T, se=F)

ggplot(variants, aes(x=freq_week_1_total,
                     y=freq_week_4,
                     colour=TryMarked.any)) +
         geom_point()+
         stat_smooth(method = "gam", fullrange = T, se=F)

```


```{r echo=F, eval=F}
#sjp.glmer(finalModel,type='pred',vars="indexical")
pdf("../results/descriptive/graphs/Indexicality_RedBrown.pdf",
    width=4.5,height=4)
ggplot(variants[variants$colourName %in% c("red","brown"),],
       aes(x=indexical,y=freq_week_4)) + geom_violin() +
  xlab("Indexicaltiy") +
  ylab("Final week frequency") +
  ggtitle("Data for RED and BROWN")
dev.off()
ggplot(variants[!variants$colourName%in% c("red","brown"),],
       aes(x=indexical,y=freq_week_4)) + geom_violin()
#sjp.glmer(finalModel,type='eff',vars="inventedBy")
#ggplot(variants, aes(log(1+freq_week_4),colour=inventedBy)) +
#  geom_density()
```


\newpage

Check model assumptions.  The model is not good at predicting higher values.

```{r}
sjp.glmer(finalModel, 'ma')
```





